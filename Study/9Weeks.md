### 📝 06. 오차 역전파 & 고급 경사 하강법 요약

* **오차 역전파 (Backpropagation)**:
    * **목적**: MLP에서 **은닉층의 가중치를 업데이트**하여 학습할 수 있게 한 획기적인 알고리즘이다.
    * **원리**: **순전파**로 오차를 구한 후, 오차를 **출력층에서 입력층 쪽으로 거꾸로 전파**하면서 각 층의 가중치를 수정한다. 이때 미분의 **체인 룰(Chain Rule)**을 적용한다.
    * **델타 식**: 오차 전파 시 사용되는 식으로, 이 덕분에 깊은 층(딥러닝)의 계산이 가능해졌다.
* **기울기 소실 문제**:
    * **원인**: 시그모이드 함수는 미분 값이 최대 0.25이므로, 여러 층을 거치면서 곱해지면 기울기가 0에 가까워져 **가중치 수정이 어려워진다**.
    * **해결**: 제프리 힌튼 교수가 **ReLU (렐루)**와 같은 새로운 **활성화 함수**를 제안하여 해결했다. ReLU는 $x>0$일 때 미분값이 1이므로 기울기가 소실되지 않고 잘 전달된다.
* **고급 경사 하강법 (Optimizer)**:
    * **확률적 경사 하강법 (SGD)**: 전체 데이터 대신 **일부 데이터**만 사용하여 업데이트 속도를 높였다.
    * **모멘텀 (Momentum)**: SGD에 **관성**을 적용하여 진동(지그재그)을 줄이고 최적 해를 향해 빠르게 전진하게 한다.
    * **아담 (Adam)**: **모멘텀**과 **RMSProp**을 합친 방법으로, 현재 **가장 많이 사용되는** 고급 최적화 알고리즘이다.
